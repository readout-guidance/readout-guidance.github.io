<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta content="Readout Guidance: Learning Control from Diffusion Features" property="og:title">
  <meta content="Readout Guidance: Learning Control from Diffusion Features" property="twitter:title">
  <meta property="og:type" content="website">
  
  <link rel="icon" type="image/png" href="assets/icon.png" />
  <title>Readout Guidance: Learning Control from Diffusion Features</title>
  <meta name="description" content="Readout Guidance is a method for controlling text-to-image diffusion models with learned signals.">
  <meta name="keywords" content="readout guidance, self guidance, diffusion hyperfeatures, diffusion features, classifier guidance, guidance, diffusion models">

  <meta content="Readout Guidance: Learning Control from Diffusion Features" property="twitter:title" />
  <meta content="Readout Guidance: Learning Control from Diffusion Features" property="twitter:description" />

  <!-- Google tag (gtag.js) -->
  <!-- <script async src=""></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-TPWJGFG8CS');
  </script> -->

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <!-- popper used for tooltips -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <!-- bootstrap styling used for carousel -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/custom.css">
  <link rel="stylesheet" href="./static/css/fonts.css">

  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/interactions.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body" style="padding: 3rem 1.5rem 0.5rem 1.5rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Readout Guidance: Learning Control<br/> from Diffusion Features</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~graceluo" target="_blank">Grace Luo</a><sup>1, 2</sup>,
            </span>
            <span class="author-block">
              <a href="http://people.eecs.berkeley.edu/~trevor" target="_blank">Trevor Darrell</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://oliverwang.nfshost.com" target="_blank">Oliver Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com" target="_blank">Dan B Goldman</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://holynski.org" target="_blank">Aleksander Holynski</a><sup>1, 2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Google Research,</span>
            <span class="author-block"><sup>2</sup>UC Berkeley</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="" class="external-link button is-normal is-rounded is-dark" disabled>
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section style="padding: 0rem 1.5rem 3rem 1.5rem;" class="section">
  <div class="container is-max-desktop">
    <div class="teaser-caption caption columns is-centered has-text-centered">
      <div>
        <!-- <img src="assets/replay.png" />
        <span class="replay">Replay Video.</span> -->
        <span><b>TL;DR: </b>We use parameter-efficient readout heads to predict useful properties and guide image generation.</span>
      </div>
    </div>
    <div class="results-grid">
      <div class="results-single">
        <video id="teaser" autoplay muted playsinline loop>
          <source src="assets/teaser.m4v" type="video/mp4">
        </video>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 style="margin-top: 2.5rem;" class="title is-4">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present Readout Guidance, a method for controlling text-to-image diffusion models with learned signals. 
            Readout Guidance uses <i>readout heads</i>, lightweight networks trained to extract signals 
            from the features of a pre-trained, frozen diffusion model at every timestep. 
            These readouts can encode single-image properties, such as pose, depth, and edges; 
            or higher-order properties that relate multiple images, such as correspondence and appearance similarity. 
            Furthermore, by comparing the readout estimates to a user-defined target, and back-propagating the gradient through the readout head, 
            these estimates can be used to guide the sampling process. Compared to prior methods for conditional generation, 
            Readout Guidance requires significantly fewer added parameters and training samples, and offers a convenient and simple recipe for 
            reproducing different forms of conditional control under a single framework, with a single architecture and sampling procedure. 
            We showcase these benefits in the applications of drag-based manipulation, identity-consistent generation, and spatially aligned control.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!--/ Abstract -->

<!-- Inuition -->
<hr>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4 is-centered has-text-centered">Extracting Readouts</h2>
    <div class="results-grid">
      <div class="results-single">
        <video autoplay muted playsinline loop>
          <source src="assets/intuition.m4v" type="video/mp4">
        </video>
      </div>
    </div>
     <div class="is-centered has-text-centered">Given a frozen, pre-trained text-to-image diffusion model, we train parameter-efficient <i>readout heads</i> to interpret relevant signals, or <i>readouts</i>, from the intermediate network features. This process is data-driven, but only requires as few as 100 paired examples, since it bootstraps the already-rich diffusion features. During sampling, a readout head can be used at any sampling timestep to query the model's current estimate of a particular property (for the image being sampled).</div>
  </div>
</section>
<!-- Inuition -->

<!-- Approach -->
<hr>
<section class="section">
  <div class="container is-max-desktop">
      <h2 class="title is-4 is-centered has-text-centered" style="margin-bottom:0;">Readout Guidance</h2>
      <br>
      <div class="columns is-vcentered">
        <div class="column is-three-fifths">
          <img src="assets/approach.png" />
        </div>
        <div class="column is-two-fifths">
          <p style="margin-left:25px;">These readouts can also be used for controlled image generation---by guiding the readout towards some desired value. Readouts for single-image concepts enable spatially aligned control, e.g., pose or depth-guided generation. Readouts for relative concepts between two images, such as appearance similarity and correspondence, enable cross-image controls, such as drag-based manipulation, identity-consistent generation, or image variations.
          </p>
        </div>
    </div>
  </div>
</section>
<!-- Approach -->

<!-- Spatially Aligned Control -->
<hr>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4 is-centered has-text-centered">Spatially Aligned Control</h2>
    <div class="columns is-centered has-text-centered">
      <img class="info-icon" src="assets/info.svg" />
      <button type="button" class="btn btn-caption btn-sm" disabled>This application uses the&nbsp;</button>
      <button type="button" class="btn btn-caption btn-sm" data-toggle="tooltip" data-placement="bottom" title="This head trains on OpenPose pose images and can be used for pose-controlled generation." disabled>Pose Head,&nbsp;</button>
      <button type="button" class="btn btn-caption btn-sm" data-toggle="tooltip" data-placement="bottom" title="This head trains on MiDaS depth images and can be used for depth-controlled generation." disabled>Depth Head,&nbsp;</button>
      <button type="button" class="btn btn-caption btn-sm" data-toggle="tooltip" data-placement="bottom" title="This head trains on HED edge images and can be used for edge-controlled generation." disabled>Edge Head</button>
    </div>
    <br>
    <div class="btn-row">
      <span>input type:<br></span>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-modality btn-select btn-active" btn-modality="pose" prompt-key="poseSpatial">pose</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-modality btn-select" btn-modality="depth" prompt-key="depthSpatial">depth</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-modality btn-select" btn-modality="edge" prompt-key="edgeSpatial">edge</button>
    </div>
    <div class="btn-row">
    <span>prompt:<br></span>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-spatial btn-select btn-active" btn-spatial="1">a group of people playing with Frisbee's on the grass</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-spatial btn-select" btn-spatial="2">two men on a tennis court shaking hands over the net</button>
    </div>
    <br>
    <div class="results-grid">
      <div class="results-single">
        <img src="assets/spatial/empty.png">
        <video class="layered-video" id="dynamic-spatial" autoplay muted playsinline loop>
          <source src="assets/spatial/pose_1.m4v" type="video/mp4">
        </video>
      </div>
      <div class="caption">
        We can take a control input and guide the readout to match for spatially controlled generation.
      </div>
    </div>
  </div>
</section>
<!-- Spatially Aligned Control -->

<!-- Drag_based Manipulation -->
<hr>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4 is-centered has-text-centered">Drag-Based Manipulation</h2>
    <div class="columns is-centered has-text-centered">
      <img class="info-icon" src="assets/info.svg" />
      <button type="button" class="btn btn-caption btn-sm" disabled>This application uses the&nbsp;</button>
    	<button type="button" class="btn btn-caption btn-sm" data-toggle="tooltip" data-placement="bottom" title="This head trains on point tracks from video and can be used to force a desired deformation." disabled>Correspondence Feature Head,&nbsp;</button>
    	<button type="button" class="btn btn-caption btn-sm" data-toggle="tooltip" data-placement="bottom" title="This head trains a contrastive similarity metric between an anchor and positive / negative images and can be used to maintain the appearance of a reference image." disabled>Appearance Similarity Head</button>
    </div>
    <br>
    <div class="title is-5">Generated Images</div>
    <div class="btn-row">
      <span>prompt:<br></span>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-prompt btn-select btn-active" btn-prompt="squirrel">photo of a squirrel with a hamburger at its feet</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-prompt btn-select" btn-prompt="bear">a bear dancing on times square</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-prompt btn-select" btn-prompt="headshot-woman">professional headshot of a woman with the eiffel tower in the background</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-prompt btn-select" btn-prompt="dog">photo of a beagle hovering mid-air</button>
    </div>
    <div class="btn-row">
    <span>edit:<br></span>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-drag btn-select btn-active" btn-drag="1">drag 1</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-drag btn-select" btn-drag="2">drag 2</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-drag btn-select" btn-drag="3">drag 3</button>
    </div>
    <br>
    <div class="results-grid">
      <div class="results-pair">
        <img id="dynamic-drag" src="assets/drag_synthetic/squirrel_1.png" />
        <img id="dynamic-result" src="assets/drag_synthetic/squirrel_1.gif" />
      </div>
      <div class="caption">
        We can take a drag input and guide for this deformation while maintaining the same appearance.
      </div>
    </div>
    <br><br><br>
    <div class="title is-5">Real Images</div>
    <div class="btn-row">
      <span>prompt:<br></span>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-prompt-real btn-select btn-active" btn-prompt-real="wolf-looking">wolf looking around</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-prompt-real btn-select" btn-prompt-real="JH_2023-09-14-1823-02">a photo of a tiger</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-prompt-real btn-select" btn-prompt-real="JH_2023-09-14-1825-41">a photo of a man holding a crocodile</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-prompt-real btn-select" btn-prompt-real="JH_2023-09-14-1838-08">a photo of a rabbit</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-prompt-real btn-select" btn-prompt-real="JH_2023-09-14-1826-07">a photo of a raccoon</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-prompt-real btn-select" btn-prompt-real="alpine-ibex">an alpine ibex with horns</button>
    </div>
    <br>
    <div class="results-grid">
      <div class="results-pair">
        <img id="dynamic-drag-real" src="assets/drag_real/wolf-looking.png" />
        <img id="dynamic-result-real" src="assets/drag_real/wolf-looking.gif" />
      </div>
      <div class="caption">
        We can do the same task on real images without per-sample optimization, where we instead use features derived from inverting the real image for guidance.
      </div>
    </div>
  </div>
</section>
<!-- Drag_based Manipulation -->

<!-- Identity Consistency -->
<hr>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4 is-centered has-text-centered">Identity Consistency</h2>
    <div class="columns is-centered has-text-centered">
      <img class="info-icon" src="assets/info.svg" />
      <button type="button" class="btn btn-caption btn-sm" disabled>This application uses the&nbsp;</button>
    	<button type="button" class="btn btn-caption btn-sm" data-toggle="tooltip" data-placement="bottom" title="This head is a variant of the appearance similarity head trained on faces and can be used to inject a specific identity." disabled>Identity Head,&nbsp;</button>
    	<button type="button" class="btn btn-caption btn-sm" data-toggle="tooltip" data-placement="bottom" title="This head is used to preserve the background of the original sampling process." disabled>Appearance Similarity Head</button>
    </div>
    <br>
    <div class="btn-row">
      <span>identity:<br></span>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-identity btn-select btn-active" btn-identity="man" prompt-key="manIdentity">person 1</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-identity btn-select" btn-identity="woman" prompt-key="womanIdentity">person 2</button>
    </div>
    <div class="btn-row">
    <span>prompt:<br></span>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-context btn-select btn-active" btn-context="1">oil painting half body portrait of a man on a city street in copenhagen</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-context btn-select" btn-context="2">oil painting of a man</button>
    </div>
    <br>
    <div class="results-grid">
      <div class="results-single">
        <img src="assets/identity/empty.png">
        <video class="layered-video" id="dynamic-identity" autoplay muted playsinline loop>
          <source src="assets/identity/man_1.m4v" type="video/mp4">
        </video>
      </div>
      <div class="caption">
        We can guide a sampling trajectory to contain a specific identity, starting from an image with a desired layout but random person.
      </div>
    </div>
  </div>
</section>
<!-- Identity Consistency -->

<!-- Image Variations -->
<hr>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4 is-centered has-text-centered">Image Variations</h2>
    <div class="columns is-centered has-text-centered">
      <img class="info-icon" src="assets/info.svg" />
      <button type="button" class="btn btn-caption btn-sm" disabled>This application uses the&nbsp;</button>
    	<button type="button" class="btn btn-caption btn-sm" data-toggle="tooltip" data-placement="bottom" title="This is the same appearance similarity head used in drag-based manipulation." disabled>Appearance Similarity Head</button>
    </div>
    <br>
    <div class="btn-row">
      <span>prompt:<br></span>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-image-var btn-select btn-active" btn-image-var="dog">a dog is walking down the street</button>
      <button type="button" onClick="setActiveButton(this)" class="btn btn-image-var btn-select" btn-image-var="astronaut">an astronaut is skiing down the hill</button>
    </div>
    <br>
    <div class="results-grid">
      <div class="results-pair">
        <img id="dynamic-image-var-ref" src="assets/image_var/dog_ref.png" />
        <img id="dynamic-image-var" src="assets/image_var/dog_0.png" />
        <div style="width:90%;">
          <b style="font-size:14px;text-align:center;"> RG Weight </b>
          <input id="dynamicSlider" type="range" value="0.0" min="0.0" max="1.0" step = "0.1" oninput="setSlider(this)">
          <b id="dynamicSliderValue">0</b>
        </div>
        <div class="caption">
          We can guide a sampling trajectory to match the appearance of a reference image, where increasing the guidance weight encourages higher similarity.
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Image Variations -->

<hr>
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-4">Acknowledgements</h2>
    <span>
      We would like to thank Stephanie Fu, Eric Wallace, Yossi Gandelsman, Dave Epstein, Ben Poole, 
      Thomas Iljic, Brent Yi, Kevin Black, Jessica Dai, Ethan Weber, Rundi Wu, Xiaojuan Wang, 
      Luming Tang, Ruiqi Gao, Jason Baldridge, Zhengqi Li, and Angjoo Kanazawa for helpful discussions and feedback.
    </span>
    <h2 class="title is-4">BibTeX</h2>
    <pre><code>
    @article{luo2023readoutguidance,
      title={Readout Guidance: Learning Control from Diffusion Features},
      author={Grace Luo and Trevor Darrell and Oliver Wang and Dan B Goldman and Aleksander Holynski},
      journal={arXiv preprint},
      year={2023}
    }
    </code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The website template is based on the
            <a href="https://nerfies.github.io">Nerfies</a>,
            <a href="https://diffusion-hyperfeatures.github.io">Diffusion Hyperfeatures</a>,
            <a href="https://dreamsim-nights.github.io">DreamSim</a>,
            <a href="https://rl-diffusion.github.io">DDPO</a>,
            and  <a href="https://dave.ml/selfguidance">Self-Guidance</a>
            project pages.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  $(function () {
    $('[data-toggle="tooltip"]').tooltip()
  });
</script>
<!-- <script>
  const replay = document.querySelector('.replay');
  replay.addEventListener('click', () => {
      const video = document.getElementById("teaser")
      video.currentTime = 0;
      video.play();
  });
</script> -->
</body>
</html>
